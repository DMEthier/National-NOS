---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Getting Started {#DataManip3}

```{r tidyr3, echo = FALSE, message = FALSE, warning = FALSE}

library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=50), tidy = FALSE)

```

## Data Download {#DataManip3.1}

The NOS data are downloaded using the `naturecounts` R package. Detailed instruction on how to use the `naturecounts` R pacakge can be found in [NatureCounts: An Introductory Tutorial](link%20to%20be%20provided) or on the [github](https://birdstudiescanada.github.io/naturecounts/index.html) repository. These instructions are not repeated here.

To access the NOS collections you must [sign up](https://www.birdscanada.org/birdmon/default/register.jsp) for a **free** account and request permission from the data custodian to access each `collection`. Once assess has been granted to can retrieve the data using the `request_id` provided. Also, don't forget to enter your personal `username` below. You will be promoted for your password.

There are many build in options which enable you to select temporal and/or spatial filters prior to download. If you want targeted data, you should add filters to the code chunk below. Because we will need additional auxiliary data from the [Bird Monitoring Data Exchange] (<https://www.birdscanada.org/birdmon/default/nc_bmde.jsp>) (BMDE) table in NatureCounts, we will want to change the `field_set` to "extend" and the `username` to your own account.

```{r downloaddata, echo = TRUE, message = FALSE}

#Example code to download individual collections

#raw.data<-nc_data_dl(collections = "ATOWLS", fields_set="extended", username ="dethier", info ="data download NOS")

#Example code to download ALL collections (still need to add ABOWLS when it become available)

raw.data<-nc_data_dl(collections = c("ATOWLS", "BCOWLS", "MBOWLS", "ONOWLS", "QCOWLS", "SKOWLS"), fields_set="extended", username ="dethier", info ="data download NOS")

```

You should save a local copy of the dataset so that you don't need to pull it from the server again.

```{r savedata, echo = FALSE, message = FALSE}

#write table to working data directory so that you have a local copy saved
write.csv(raw.data, "data/raw.owls.data.csv")

#to read saved data file into R ensure that you 
raw.data<-read.csv("data/raw.owls.data.csv")

```

## Select {#DataManip3.2}

Now that the data are downloaded we will want to select the columns needed for the analysis. You may not have all the variables below if you didn't change the `field_set` to "extend". That is OK! You may not need all the auxiliary data for your analytical purposes.

```{r dataselect1, echo = TRUE, message = FALSE}

in.data<-raw.data %>% select(SamplingEventIdentifier, SurveyAreaIdentifier,RouteIdentifier, Locality, SiteCode, collection, survey_day, survey_month, survey_year, survey_week, ProtocolCode, CollectorNumber, EffortUnits1, EffortMeasurement1, EffortUnits3, EffortMeasurement3, EffortUnits5, EffortMeasurement5, EffortUnits11, EffortMeasurement11, EffortUnits14, EffortMeasurement14, species_id, CommonName, ScientificName, latitude, longitude, bcr, StateProvince, ObservationDescriptor, ObservationCount, ObservationDescriptor2, ObservationCount2,ObservationDescriptor3, ObservationCount3)
```

Notice here that we don't keep all the ObservationCount fields. There are more that could be retained that capture owls call during each broadcast period.

For the purposes of this analysis, we keep ObservationCount2 + ObservationCount3 = Number of owls detected before call playback is used (i.e., silent listening period only). This is nationally standardized. Some protocols do not have call playback, and we can therefore use ObservationCount (totals) for the analysis. 

If you want to keep all counts, inclusive of the silent listening + call playback, keep ObservationCount.  

# Filter {#DataManip3.3}

Filter out the data we know we don't want before proceeding.

```{r datafilter1, echo = TRUE, message = FALSE}

#Drop Newfoundland & Labrador based on StateProvince 
in.data<-in.data %>% filter (StateProvince!="Newfoundland") %>% droplevels()

#Drop Northwest Terriroties based on route identifier, survey started in 2018, and therefore not enough data yet for a national assessment
in.data<-filter(in.data, !grepl("NT", RouteIdentifier)) %>%  droplevels()

#Remove surveys with missing Month, Day, Year
in.data<-in.data %>% filter (!is.na(survey_day), !is.na(survey_month), !is.na(survey_year))

#Some Ontario Routes only have lat/long for the start point, so this can't be used for the national assessment. 
#Remove surveys with missing lat long
#in.data<-in.data %>% filter (!is.na(latitude), !is.na(longitude))

#Remove survyes with missing protocol ID 
in.data<-in.data %>% filter (!is.na(ProtocolCode))

#Remove surveys with NOCTOWLS protocol ID
in.data<-in.data %>% filter (ProtocolCode != "NOCTOWLS")
```

If you are using the BC data and want to assign `ProtocolCode` to differentiate regions. First you need to assign BC new protocol ID since this is not done in the underlying database to reflect difference in data collection between region.

```{r protocol filter, echo = TRUE, message = FALSE}

#Use this previously loaded table: Regions_BCY.csv

in.data<-left_join(in.data, BCregion, by="RouteIdentifier")
in.data$ProtocolCode <- ifelse(in.data$collection=="BCOWLS", in.data$protocol_id_new, in.data$ProtocolCode)
in.data<-in.data %>% select(-Timing.Region, -protocol_id_new)
in.data<-in.data %>% filter (!is.na(ProtocolCode))
```

You may want to fix some data inconsistencies in StateProvince naming. However, you may want to use `collection` or `ProtocolCode` rather than `StateProvince` to do the analysis since there are some points that cross the provincial boundaries. Something to consider. 

```{r datafilter2, echo = TRUE, message = FALSE}

in.data$StateProvince[in.data$StateProvince  == "Ontario"]  <-  "ON"
in.data$StateProvince[in.data$StateProvince  == "British Columbia and Yukon"]  <-  "BCY"
in.data$StateProvince[in.data$StateProvince  == "ME"]  <- "QC"
in.data$StateProvince[in.data$StateProvince  == "NL"]  <- "QC"
in.data$StateProvince[in.data$StateProvince  == "Manitoba"]  <- "MB"
in.data$StateProvince[in.data$StateProvince  == "MN"]  <- "MB"

```

Next we add a day-of-year column using the `format_dates` [helper function](https://birdstudiescanada.github.io/naturecounts/reference/format_dates.html).

```{r format dates, echo = TRUE, message = FALSE}

#create a doy variable with the `format_dates` NatureCounts function. 
in.data<-format_dates(in.data)

```

Create the output tables for writing the data cleaning results
```{r output tables}

Events<- as.data.frame(matrix(data = NA, nrow = 1, ncol = 9, byrow = FALSE, dimnames = NULL))
   names(Events) <- c("SiteCode", "RouteIdentifier", "survey_year", "CollectorNumber", "nstop", "StateProvince", "latitude", "longitude", "bcr")

#only need to create the table once per analysis   
write.table(Events, file = paste(out.dir,  "Events",".csv", sep = ""), row.names = FALSE, append = FALSE, quote = FALSE, sep = ",")

Owls<- as.data.frame(matrix(data = NA, nrow = 1, ncol = 11, byrow = FALSE, dimnames = NULL))
   names(Owls) <- c("SiteCode", "RouteIdentifier", "survey_year", "CollectorNumber", "collection", "ProtocolCode", "doy",  "CommonName", "species_id", "ObservationCount", "StateProvince")

#only need to create the table once per analysis   
write.table(Owls, file = paste(out.dir,  "OwlDataClean",".csv", sep = ""), row.names = FALSE, append = FALSE, quote = FALSE, sep = ",")

```


In this step, we filter the data based on the `Analysis Parameters` file. For example, we will remove surveys done in inappropriate weather conditions. While we can also remove surveys done outside the appropriate survey window (min.day, max.day) it was decided by the national NOS committee not to proceed with this step since survey timing of often tweaked to accommodate weather.

```{r dataclean, echo = TRUE, message = FALSE}

#loop through each row in the analysis parameter files is doing multiple protocol
for(k in 1:nrow(anal.param)) {
  
k<-13 # manually specify the row in the Analysis Parameters file to be run. Currently set to QC

max.yr <- 2023 #set to the maximum year of current data

#Note: data from 2020 will be incomplete for most provinces due to COVID19. 

#Pull the protocol-specific data you need for filtering from the Analysis Parameters file

protocol_id<-anal.param[k,"protocol_id"]
collection<-anal.param[k, "collection"]
min.yr <- anal.param[k,"min.year"] 
min.doy <- anal.param[k,"min.doy"]
max.doy <- anal.param[k,"max.doy"]
temp <- anal.param[k,"temp"]
obs<-anal.param[k,"obs"]

#adjust to add a 7 day buffer on either side

if(protocol_id=="35"){ #Add buffer to QC data for cleaning. Others may also request this filter be adjusted. 
min.doy <- min.doy-7
max.doy <- max.doy+7
}

#subset data based on protocol_ID 
dat<-NULL
dat<-in.data %>% filter(ProtocolCode == protocol_id)
dat$ObservationCount[is.na(dat$ObservationCount)] <- 0
dat$ObservationCount<-as.numeric(dat$ObservationCount)

#reassign routes to correct provinces and add SiteCode to QC
if(protocol_id=="35"){
dat$StateProvince[dat$StateProvince  == "ON"]  <- "QC"  

dat<-dat %>% separate(SamplingEventIdentifier, c("del1", "del2", "StopNo"), sep="-", remove=FALSE) %>% dplyr::select (-del1, -del2) %>% mutate(SiteCode2= paste(RouteIdentifier, StopNo, sep="-")) %>% select(-StopNo)

dat<-dat %>% mutate(SiteCode = ifelse(is.na(SiteCode), SiteCode2, SiteCode)) %>% select(-SiteCode2)

}

#Subset data to specified year range
#max.yr <- anal.param[k,"max.year"] #manually set to 2021 at this time

dat <- dat %>% filter(survey_year >= min.yr & survey_year <= max.yr)

##____________________________________________________________________
#Create a dataframe with a single lat long per route ID (what the first stop in a route). This is necessary because some Ontario routes only have a lat long for the first stop in a route. 
loc.dat <-NULL #clear old

#Using SamplingEvent
loc.dat<-dat %>% separate(SamplingEventIdentifier, c("del1", "del2", "Stop"), sep="-", remove=FALSE) %>% select (-del1, -del2)

loc.dat<-loc.dat %>% filter(latitude!="NA") %>% arrange(Stop) %>% distinct(RouteIdentifier, .keep_all = TRUE) %>% select(RouteIdentifier, latitude, longitude, bcr)

#write.csv(loc.dat, "output/Map.csv")

##____________________________________________________________________
#Because in the early years the Ontario owls program ran upwards of 4 surveys/route/year, the duplicates will need to be removed. We run this script for all province since there are some other duplicate routes that have made their way into the database. 

#We select the last survey date for those that have multiple surveys per route within a year

#first filter by the min and max survey doy to capture the surveys that are done consistently with current protocol
dat <- dat %>% filter(doy >= min.doy & doy <= max.doy)
  
#if multiple surveys done in a year, keep the first survey. 
dat <- dat %>% group_by(SiteCode, survey_year)%>%
  slice_min(survey_day)


##______________________________________________________________
#Filtering the data to standardized detectability. 

#Subset data to specified doy range
#This really butcher some of the datasets. I think we can justify turning this filter off. 
#dat <- dat %>% filter(doy >= min.doy & doy <= max.doy)

#Subset data to minimum temperature using the start temp at the beginning of the survey. We will also keep NAs and assume these dataset are ok. This will help us preserve data. Could change moving forward. 
dat <- dat %>% filter(EffortMeasurement3 >= temp | is.na(EffortMeasurement3))

#Subset data to remove data collected when start precip > 1, start wind > 3 or noise > 3. We will also keep NAs and assume they are collected under appropriate conditions to help preserve data. Could change moving forward. Currently removed noise.   
dat<-dat %>% filter(EffortMeasurement1 <= 3 | is.na(EffortMeasurement1)) #Start wind
dat<-dat %>% filter(EffortMeasurement5 <= 1 | is.na(EffortMeasurement5)) #Start precip
#dat<-dat %>% filter(EffortMeasurement11 <= 3 | #is.na(EffortMeasurement11)) #Noise Level

#Now we want to remove repeat counts of individuals between stops. Not all region record this, but many do.
dat<-dat %>% filter(is.na(EffortMeasurement14)|EffortMeasurement14==0) #Repeats removed

##____________________________________________________________________
#The number of stops on a route within a year is used an an effort covariate in the model
stop.year<-dat %>% group_by(RouteIdentifier, survey_year) %>% summarize(nstop=n_distinct(SurveyAreaIdentifier))
dat<-left_join(dat, stop.year, by=c("RouteIdentifier", "survey_year"))

##___________________________________________________________________
#Filter incomplete routes, which are defined here as routes that had less than 5 stops complete in a year. 

dat<- dat %>% filter(nstop>=5)

##____________________________________________________________________
#Now that the surveys are removed that were done in inappropriate environmental conditions we can make a list of unique sampling events to use for zero-filling species-specific data frames. We don't zero-fill the entire data frame for space issues. 

#Note: Since our response variable in the analysis is counts at the route-level, the sampling event is a route within a year. If the analysis is changed to be at the stop level, you will want to include `month` and `day` in the code below.   

event.data <-NULL #clear old
event.data <- dat %>%
  select(RouteIdentifier, survey_year, CollectorNumber, nstop, StateProvince) %>%  
  distinct() %>%
  ungroup() %>%
  as.data.frame()

#merge with the loc.data to assign unique lat long to each route
event.data<-left_join(event.data, loc.dat, by=c("RouteIdentifier"))

##____________________________________________________________________
#Because we have different data collection methods, the response variable for each protocol may be ObservationCount2+ObservationCount3 = ObservationCount OR ObservationCount depending on how the data are recorded. The Analysis Parameters files will specify this for each protocol under the `obs` column. 

#Use this code is you would like to use just the date collected during the first 2 min
#if(obs=="ObservationCount1"){
#dat$ObservationCount2<-ifelse(is.na(dat$ObservationCount2), 0, 1)
#dat$ObservationCount3<-ifelse(is.na(dat$ObservationCount3), 0, 1)
#dat<- dat %>% mutate(ObservationCount = ifelse (ObservationCount2==1 | ObservationCount3==1, 1, 0))    
#}

#now we no longer need these fields
dat<-dat %>% select(-ObservationCount2, -ObservationCount3, -ObservationDescriptor2, -ObservationDescriptor3)

##Remove rare species
#Species must be detected on at least half of all years surveyed.  
min.yrs.detect<-trunc(length(unique(dat$survey_year))/2)

df.nyears <- NULL #clear old
df.nyears <- dat %>%
  filter(ObservationCount > 0) %>%
  select(survey_year, CommonName) %>%
  distinct() %>%
  group_by(CommonName) %>%
  summarize(nyears = n()) %>%
  filter(nyears >= min.yrs.detect)%>%
  as.data.frame() %>% 
  filter(CommonName != "owl sp.")

dat <- left_join(df.nyears, dat, by = c("CommonName")) %>%  select(-nyears)

#Species must be detected with a mean of ~5 per years to be included in the analysis. 

df.abund <- NULL #clear old
df.abund <- dat %>%
  group_by(CommonName, survey_year) %>%
  summarize(count = sum(ObservationCount)) %>% 
  summarize(meanCount = mean(count, na.rm = TRUE)) %>% 
  filter(meanCount >= 5)%>%
  as.data.frame()

dat <- left_join(df.abund, dat, by = c("CommonName")) %>%
  select(-meanCount)%>%
  as.data.frame()

#print the final data to file
#write.csv(dat, paste(out.dir, collection, ".", protocol_id, ".csv", sep=""))

Owls<-NULL
Owls<-dat %>% select(SiteCode, RouteIdentifier, survey_year, CollectorNumber, collection, ProtocolCode, doy,  CommonName, species_id, ObservationCount, StateProvince)

write.table(Owls, file = paste(out.dir,  "OwlDataClean.csv", sep = ""), row.names = FALSE, append = TRUE, quote = FALSE, sep = ",", col.names = FALSE)

#print the event.data to file 
#write.csv(event.data, paste(out.dir, collection, ".", protocol_id, ".EventData.csv", sep=""))

Events<-NULL
Events<-event.data 
write.table(Events, file = paste(out.dir,  "Events",".csv", sep = ""), row.names = FALSE, append = TRUE, quote = FALSE, sep = ",", col.names = FALSE)

} #end loop

```

---
title: "RouteSum"
author: "Danielle Ethier"
date: "2023-11-11"
output: html_document
---

Route level summary of owl data for student projects

```{r setup, include=FALSE}

library(naturecounts)
library(tidyverse)
library(lubridate)
library(reshape)


dir.create("data")
dir.create("output")

out.dir <- paste("output/")
dat.dir <- paste("data/")

#These tables are available on Github 

#Main Analysis Parameters file 
anal.param<-read.csv("Analysis Parameters.csv")
#remove sites that we don't want for the national analysis
anal.param<-anal.param %>% filter(!is.na(protocol_id)) %>% filter(protocol_id!=29)

#BC specific
BCregion<-read.csv("Regions_BCY.csv")

```

```{ r data}

#Select your collection
raw.data<-nc_data_dl(collections = c("ONOWLS"), fields_set="extended", username ="dethier", info ="data download NOS")

in.data<-raw.data %>% select(SamplingEventIdentifier, SurveyAreaIdentifier,RouteIdentifier, Locality, SiteCode, collection, survey_day, survey_month, survey_year, survey_week, ProtocolCode, CollectorNumber, EffortUnits1, EffortMeasurement1, EffortUnits3, EffortMeasurement3, EffortUnits5, EffortMeasurement5, EffortUnits11, EffortMeasurement11, EffortUnits14, EffortMeasurement14, species_id, CommonName, ScientificName, latitude, longitude, bcr, StateProvince, ObservationDescriptor, ObservationCount, ObservationDescriptor2, ObservationCount2,ObservationDescriptor3, ObservationCount3)

#Drop Newfoundland & Labrador based on StateProvince 
in.data<-in.data %>% filter (StateProvince!="Newfoundland") %>% droplevels()

#Drop Northwest Terriroties based on route identifier, survey started in 2018, and therefore not enough data yet for a national assessment
in.data<-filter(in.data, !grepl("NT", RouteIdentifier)) %>%  droplevels()

#Remove surveys with missing Month, Day, Year
in.data<-in.data %>% filter (!is.na(survey_day), !is.na(survey_month), !is.na(survey_year))

#Some Ontario Routes only have lat/long for the start point, so this can't be used for the national assessment. 
#Remove surveys with missing lat long
#in.data<-in.data %>% filter (!is.na(latitude), !is.na(longitude))

#Remove survyes with missing protocol ID 
in.data<-in.data %>% filter (!is.na(ProtocolCode))

#Remove surveys with NOCTOWLS protocol ID
in.data<-in.data %>% filter (ProtocolCode != "NOCTOWLS")

in.data<-left_join(in.data, BCregion, by="RouteIdentifier")
in.data$ProtocolCode <- ifelse(in.data$collection=="BCOWLS", in.data$protocol_id_new, in.data$ProtocolCode)
in.data<-in.data %>% select(-Timing.Region, -protocol_id_new)
in.data<-in.data %>% filter (!is.na(ProtocolCode))

in.data$StateProvince[in.data$StateProvince  == "Ontario"]  <-  "ON"
in.data$StateProvince[in.data$StateProvince  == "British Columbia and Yukon"]  <-  "BCY"
in.data$StateProvince[in.data$StateProvince  == "ME"]  <- "QC"
in.data$StateProvince[in.data$StateProvince  == "NL"]  <- "QC"
in.data$StateProvince[in.data$StateProvince  == "Manitoba"]  <- "MB"
in.data$StateProvince[in.data$StateProvince  == "MN"]  <- "MB"

in.data<-format_dates(in.data)

#Create tables
Events<- as.data.frame(matrix(data = NA, nrow = 1, ncol = 9, byrow = FALSE, dimnames = NULL))
   names(Events) <- c("SiteCode", "RouteIdentifier", "survey_year", "CollectorNumber", "nstop", "StateProvince", "latitude", "longitude", "bcr")

#only need to create the table once per analysis   
write.table(Events, file = paste(out.dir,  "Events",".csv", sep = ""), row.names = FALSE, append = FALSE, quote = FALSE, sep = ",")

Owls<- as.data.frame(matrix(data = NA, nrow = 1, ncol = 10, byrow = FALSE, dimnames = NULL))
   names(Owls) <- c("SiteCode", "RouteIdentifier", "survey_year", "CollectorNumber", "collection", "ProtocolCode", "doy",  "CommonName", "ObservationCount", "StateProvince")

#only need to create the table once per analysis   
write.table(Owls, file = paste(out.dir,  "OwlCleanSum",".csv", sep = ""), row.names = FALSE, append = FALSE, quote = FALSE, sep = ",")


```


```{r data process}

for(k in 1:nrow(anal.param)) {
  
k<-12 # manually specify the row in the Analysis Parameters file to be run. Currently set to QC

max.yr <- 2023 #set to the maximum year of current data

#Note: data from 2020 will be incomplete for most provinces due to COVID19. At the time of writing, not all 2021 data were proofed in NatureCounts and may be missing

#Pull the protocol-specific data you need for filtering from the Analysis Parameters file

protocol_id<-anal.param[k,"protocol_id"]
collection<-anal.param[k, "collection"]
min.yr <- anal.param[k,"min.year"] 
min.doy <- anal.param[k,"min.doy"]
max.doy <- anal.param[k,"max.doy"]
temp <- anal.param[k,"temp"]
obs<-anal.param[k,"obs"]

#adjust to add a 7 day buffer on either side

if(protocol_id=="35"){ #Add buffer to QC data for cleaning. Others may also request this filter be adjusted. 
min.doy <- min.doy-7
max.doy <- max.doy+7
}

#subset data based on protocol_ID 
dat<-NULL
dat<-in.data %>% filter(ProtocolCode == protocol_id)
dat$ObservationCount[is.na(dat$ObservationCount)] <- 0
dat$ObservationCount<-as.numeric(dat$ObservationCount)

#reassign routes to correct provinces and add SiteCode to QC
if(protocol_id=="35"){
dat$StateProvince[dat$StateProvince  == "ON"]  <- "QC"  

dat<-dat %>% separate(SamplingEventIdentifier, c("del1", "del2", "StopNo"), sep="-", remove=FALSE) %>% dplyr::select (-del1, -del2) %>% mutate(SiteCode2= paste(RouteIdentifier, StopNo, sep="-")) %>% select(-StopNo)

dat<-dat %>% mutate(SiteCode = ifelse(is.na(SiteCode), SiteCode2, SiteCode)) %>% select(-SiteCode2)

}

##____________________________________________________________________
#Create a dataframe with a single lat long per route ID (what the first stop in a route). This is necessary because some Ontario routes only have a lat long for the first stop in a route. 
loc.dat <-NULL #clear old

#Using SamplingEvent
loc.dat<-dat %>% separate(SamplingEventIdentifier, c("del1", "del2", "Stop"), sep="-", remove=FALSE) %>% select (-del1, -del2)

loc.dat<-loc.dat %>% filter(latitude!="NA") %>% arrange(Stop) %>% distinct(RouteIdentifier, .keep_all = TRUE) %>% select(RouteIdentifier, latitude, longitude, bcr)

write.csv(loc.dat, "output/Map.csv")

##____________________________________________________________________
#Because in the early years the Ontario owls program ran upwards of 4 surveys/route/year, the duplicates will need to be removed. We run this script for all province since there are some other duplicate routes that have made their way into the database. 

#We select the last survey date for those that have multiple surveys per route within a year

#first filter by the min and max survey doy to capture the surveys that are done consistently with current protocol
dat <- dat %>% filter(doy >= min.doy & doy <= max.doy)
  
#if multiple surveys done in a year, keep the first survey. 
dat <- dat %>% group_by(SiteCode, survey_year)%>%
  slice_min(survey_day)

##____________________________________________________________________
#The number of stops on a route within a year is used an an effort covariate in the model
stop.year<-dat %>% group_by(RouteIdentifier, survey_year) %>% summarize(nstop=n_distinct(SurveyAreaIdentifier))
dat<-left_join(dat, stop.year, by=c("RouteIdentifier", "survey_year"))

##____________________________________________________________________
#Now that the surveys are removed that were done in inappropriate environmental conditions we can make a list of unique sampling events to use for zero-filling species-specific data frames. We don't zero-fill the entire data frame for space issues. 

#Note: Since our response variable in the analysis is counts at the route-level, the sampling event is a route within a year. If the analysis is changed to be at the stop level, you will want to include `month` and `day` in the code below.   

event.data <-NULL #clear old
event.data <- dat %>%
  select(RouteIdentifier, survey_year, CollectorNumber, nstop, StateProvince) %>%  
  distinct() %>%
  ungroup() %>%
  as.data.frame()

#merge with the loc.data to assign unique lat long to each route
event.data<-left_join(event.data, loc.dat, by=c("RouteIdentifier"))

#now we no longer need these fields
dat<-dat %>% select(-ObservationCount2, -ObservationCount3, -ObservationDescriptor2, -ObservationDescriptor3)

#print the final data to file
#write.csv(dat, paste(out.dir, collection, ".", protocol_id, ".csv", sep=""))

Owls<-NULL
Owls<-dat %>% select(SiteCode, RouteIdentifier, survey_year, CollectorNumber, collection, ProtocolCode, doy,  CommonName, ObservationCount, StateProvince)

write.table(Owls, file = paste(out.dir,  "OwlCleanSum.csv", sep = ""), row.names = FALSE, append = TRUE, quote = FALSE, sep = ",", col.names = FALSE)

#print the event.data to file 
#write.csv(event.data, paste(out.dir, collection, ".", protocol_id, ".EventData.csv", sep=""))

Events<-NULL
Events<-event.data 
write.table(Events, file = paste(out.dir,  "Events",".csv", sep = ""), row.names = FALSE, append = TRUE, quote = FALSE, sep = ",", col.names = FALSE)

} #end loop


```

```{r routesum}
obs<-read.csv("output/OwlCleanSum.csv")
event<-read.csv("output/Events.csv")

#select desired data columns for analysis
obs<-obs %>% select("CommonName", "RouteIdentifier", "collection", "ProtocolCode", "survey_year", "doy", "ObservationCount")

route.sum<-obs %>% group_by(survey_year, RouteIdentifier, CommonName) %>% summarise(count = sum(ObservationCount))
route.sum<-cast(route.sum, RouteIdentifier+CommonName~survey_year, value="count")

write.csv(route.sum, "output/RouteSumOwls.csv")

```
